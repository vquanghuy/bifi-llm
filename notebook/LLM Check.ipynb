{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc5440b-b087-4c53-9f0f-e6e1802a7b26",
   "metadata": {},
   "source": [
    "# System Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fd7f24-805e-475d-a83c-2c7b578e5000",
   "metadata": {},
   "source": [
    "- Python 3.10\n",
    "- PyTorch 2.5.0\n",
    "- Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d09578-3c1f-4b83-9439-1cc5ab162720",
   "metadata": {},
   "source": [
    "## Additional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e8267e-4aff-4667-b9f6-9246865a4667",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ae8ec-7782-4afb-9857-333bf7572eab",
   "metadata": {},
   "source": [
    "Hugging Face Transformers at https://huggingface.co/docs/transformers/installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efec68e5-1083-4e25-9aa4-cf4acfd547a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flax\n",
      "  Using cached flax-0.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.46.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting gdown\n",
      "  Using cached gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jax>=0.4.27 (from flax)\n",
      "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting msgpack (from flax)\n",
      "  Using cached msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting optax (from flax)\n",
      "  Using cached optax-0.2.3-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting orbax-checkpoint (from flax)\n",
      "  Using cached orbax_checkpoint-0.7.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorstore (from flax)\n",
      "  Downloading tensorstore-0.1.67-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1 (from flax)\n",
      "  Downloading rich-13.9.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from flax) (4.11.0)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from flax) (6.0.2)\n",
      "Requirement already satisfied: filelock in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Using cached tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from accelerate) (2.5.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from gdown) (4.12.3)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.4.27->flax)\n",
      "  Downloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
      "Collecting ml-dtypes>=0.4.0 (from jax>=0.4.27->flax)\n",
      "  Using cached ml_dtypes-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting opt-einsum (from jax>=0.4.27->flax)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting scipy>=1.10 (from jax>=0.4.27->flax)\n",
      "  Using cached scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1->flax)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from rich>=11.1->flax) (2.18.0)\n",
      "Requirement already satisfied: networkx in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Collecting sympy==1.13.1 (from torch>=1.10.0->accelerate)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.6)\n",
      "Collecting absl-py>=0.7.1 (from optax->flax)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting chex>=0.1.86 (from optax->flax)\n",
      "  Using cached chex-0.1.87-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting etils[epy] (from optax->flax)\n",
      "  Using cached etils-1.10.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: nest_asyncio in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from orbax-checkpoint->flax) (1.6.0)\n",
      "Collecting protobuf (from orbax-checkpoint->flax)\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting humanize (from orbax-checkpoint->flax)\n",
      "  Using cached humanize-4.11.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Collecting toolz>=0.9.0 (from chex>=0.1.86->optax->flax)\n",
      "  Using cached toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1->flax)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting importlib_resources (from etils[epath,epy]->orbax-checkpoint->flax)\n",
      "  Using cached importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting zipp (from etils[epath,epy]->orbax-checkpoint->flax)\n",
      "  Using cached zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/huyvu/miniconda3/envs/bifi-llm/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Using cached flax-0.10.0-py3-none-any.whl (420 kB)\n",
      "Downloading transformers-4.46.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "Using cached gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading huggingface_hub-0.26.1-py3-none-any.whl (447 kB)\n",
      "Downloading jax-0.4.35-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "Downloading rich-13.9.3-py3-none-any.whl (242 kB)\n",
      "Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Using cached tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "Using cached optax-0.2.3-py3-none-any.whl (289 kB)\n",
      "Using cached orbax_checkpoint-0.7.0-py3-none-any.whl (279 kB)\n",
      "Downloading tensorstore-0.1.67-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached chex-0.1.87-py3-none-any.whl (99 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl (87.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached ml_dtypes-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "Using cached scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "Using cached humanize-4.11.0-py3-none-any.whl (128 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached toolz-1.0.0-py3-none-any.whl (56 kB)\n",
      "Using cached etils-1.10.0-py3-none-any.whl (164 kB)\n",
      "Using cached importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Using cached zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
      "Installing collected packages: zipp, tqdm, toolz, sympy, scipy, safetensors, regex, protobuf, opt-einsum, msgpack, ml-dtypes, mdurl, importlib_resources, humanize, fsspec, etils, absl-py, tensorstore, markdown-it-py, jaxlib, huggingface-hub, tokenizers, rich, jax, gdown, accelerate, transformers, orbax-checkpoint, chex, optax, flax\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed absl-py-2.1.0 accelerate-1.0.1 chex-0.1.87 etils-1.10.0 flax-0.10.0 fsspec-2024.10.0 gdown-5.2.0 huggingface-hub-0.26.1 humanize-4.11.0 importlib_resources-6.4.5 jax-0.4.35 jaxlib-0.4.35 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.0 msgpack-1.1.0 opt-einsum-3.4.0 optax-0.2.3 orbax-checkpoint-0.7.0 protobuf-5.28.3 regex-2024.9.11 rich-13.9.3 safetensors-0.4.5 scipy-1.14.1 sympy-1.13.1 tensorstore-0.1.67 tokenizers-0.20.1 toolz-1.0.0 tqdm-4.66.5 transformers-4.46.0 zipp-3.20.2\n"
     ]
    }
   ],
   "source": [
    "!pip install flax transformers accelerate gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6feffa-e141-4a94-9f24-8d06eb5d1271",
   "metadata": {},
   "source": [
    "# Load and check LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a84ad655-40cf-4c48-b830-05e847d16372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a code error generator. You will be provided with a number as input. Generate that many unique JavaScript code snippets, each containing at least one syntax error.  Output only the code snippets.\"},\n",
    "    {\"role\": \"user\", \"content\": \"5\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185461fc-943f-42d8-8637-8e5100407f6a",
   "metadata": {},
   "source": [
    "## Llama 3.1 8B Instruct\n",
    "\n",
    "Details: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aeedf03b-3048-4d71-a132-6d59139a009c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cccd21cebaa4d568f6580126d3d5fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipe = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a765d653-eb5f-45fe-90a2-482edc31f03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"```javascript\\n// Code Snippet 1\\nlet x = 5;\\ny = x + 10; // Missing 'let' keyword\\nconsole.log(y);\\n```\\n\\n```javascript\\n// Code Snippet 2\\nfunction add(a, b) {\\n  return a + b;\\n}\\nconsole.log(add(5, 10); // Missing closing parenthesis\\n```\\n\\n```javascript\\n// Code Snippet 3\\nlet arr = [1, 2, 3];\\nconsole.log(arr[5]); // Array index out of bounds\\n```\\n\\n```javascript\\n// Code Snippet 4\\nclass Person {\\n  constructor(name) {\\n    this.name = name;\\n  }\\n}\\nlet person = new Person('John'); // Missing semicolon after class declaration\\nconsole.log(person.name);\\n```\\n\\n```javascript\\n// Code Snippet 5\\nlet obj = { name: 'John' };\\nconsole.log(obj.name + 10); // Attempting to add a string and a number\\n```\"}\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    pad_token_id=50256\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b87f20-d3db-404a-801d-f01ff002cce9",
   "metadata": {},
   "source": [
    "## Llama 3.2 3B Instruct\n",
    "\n",
    "Details: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85bb8526-46fe-40cf-9d74-24b6c4fb6ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4741876d81ba4c45aef86b77abd11ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "pipe = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22ea9bd6-fa0f-42fd-992a-ce798fdf5b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"```javascript\\nfunction add(a, b) {\\n    return a + b;\\n}\\n\\nconsole.log(add(5, 10)); // SyntaxError: Unexpected token 'console.log'\\nconsole.log(add(5, 10)); // SyntaxError: Unexpected token 'console.log'\\nconsole.log(add(5, 10)); // SyntaxError: Unexpected token 'console.log'\\nconsole.log(add(5, 10)); // SyntaxError: Unexpected token 'console.log'\\nconsole.log(add(5, 10)); // SyntaxError: Unexpected token 'console.log'\\n```\"}\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    pad_token_id=50256\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac1cc14-d318-40f5-b352-320a2dc5b9d5",
   "metadata": {},
   "source": [
    "## CodeLlama 7b Instruct\n",
    "\n",
    "Details https://huggingface.co/meta-llama/CodeLlama-7b-Instruct-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68b454cd-9f90-470a-9bcd-c5616a225f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0986bdf39543a6b9cbef5461418beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/CodeLlama-7b-Instruct-hf\"\n",
    "\n",
    "pipe = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f618479b-de07-443f-b58a-f76e713a66ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    pad_token_id=50256\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181a8a40-b782-4230-9cd0-2271276477f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76884859-7b3d-4cac-9bb8-0432573ca212",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### TFix dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c02265-6560-4d86-9114-2219bbd208b8",
   "metadata": {},
   "source": [
    "https://drive.google.com/file/d/1CtfnYaVf-q6FZP5CUM4Wh7ofpp8b9ajW/view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91802d0-18b1-458b-bf9e-a503e65672c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9b5448-213a-4eb9-92d9-e0a94532e7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4773e5-d9f8-4cce-b7fe-fbc93c2181af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:  25%|█████████████████████████▊                                                                             | 1/4 [07:46<23:20, 466.84s/it]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "hf_token = \"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\"\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a46504df-ee6c-4c88-81b5-f0bd55796e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[cli] in /opt/conda/lib/python3.11/site-packages (0.26.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface_hub[cli]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub[cli]) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub[cli]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub[cli]) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface_hub[cli]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub[cli]) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub[cli]) (4.11.0)\n",
      "Collecting InquirerPy==0.3.4 (from huggingface_hub[cli])\n",
      "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface_hub[cli])\n",
      "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.43)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub[cli]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub[cli]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub[cli]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub[cli]) (2024.7.4)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.5)\n",
      "Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device: '/tmp/tmp11krsk0q'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub[\"cli\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c2af4db-b3d1-4b18-b3cd-14f19b84bd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface/hub\n"
     ]
    }
   ],
   "source": [
    "from transformers import file_utils\n",
    "print(file_utils.default_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c335690b-b5a5-4600-bb34-c2038a7aabdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
