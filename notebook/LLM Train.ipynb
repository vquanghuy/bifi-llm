{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad2d426-4623-4266-9c25-f4a46c9fb33f",
   "metadata": {},
   "source": [
    "# Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff25070e-358d-4e64-8b50-e8888adba458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils.data_reader import GetDataAsPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc537045-7c0e-49bf-ab3d-102ddeba93ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set HF token as environment variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "730c71ef-99aa-45f6-92dc-c0695cf979de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare token\n",
    "torch.cuda.empty_cache()\n",
    "hf_token = os.environ.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb17e6db-3c25-444b-b0b1-2f7f5d4ec270",
   "metadata": {},
   "source": [
    "# Prepare helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "232f04ab-5242-462c-813e-11a6b351c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def extract_warning_types(data):\n",
    "    all_warnings = []\n",
    "    for sample in data:\n",
    "        if sample.linter_report.rule_id not in all_warnings:\n",
    "            all_warnings.append(sample.linter_report.rule_id)\n",
    "    return all_warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd0ac8e-2bb8-4978-884f-be954675cc8a",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = GetDataAsPython(\"../data/data_autofix_tracking_repo_specific_final.json\")\n",
    "data_eslint = GetDataAsPython(\"../data/data_autofix_tracking_eslint_final.json\")\n",
    "data += data_eslint\n",
    "\n",
    "all_warning_types = extract_warning_types(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f23fb11f-c9b1-4b55-a985-3e21de842f79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no-invalid-this',\n",
       " 'no-throw-literal',\n",
       " 'no-new-wrappers',\n",
       " 'guard-for-in',\n",
       " 'no-new-object',\n",
       " 'comma-style',\n",
       " 'prefer-spread',\n",
       " 'no-caller',\n",
       " 'no-extra-bind',\n",
       " 'no-array-constructor',\n",
       " 'prefer-rest-params',\n",
       " 'generator-star-spacing',\n",
       " 'no-this-before-super',\n",
       " 'no-extend-native',\n",
       " 'no-undef',\n",
       " 'no-useless-escape',\n",
       " 'no-dupe-keys',\n",
       " 'no-console',\n",
       " 'no-constant-condition',\n",
       " 'no-duplicate-case',\n",
       " 'no-empty',\n",
       " 'no-extra-semi',\n",
       " 'no-redeclare',\n",
       " 'no-cond-assign',\n",
       " 'no-extra-boolean-cast',\n",
       " 'no-fallthrough',\n",
       " 'no-unreachable',\n",
       " 'valid-typeof',\n",
       " 'no-unsafe-finally',\n",
       " 'no-unused-vars',\n",
       " 'no-debugger',\n",
       " 'no-unsafe-negation',\n",
       " 'no-case-declarations',\n",
       " 'no-self-assign',\n",
       " 'no-process-exit',\n",
       " 'no-inner-declarations',\n",
       " 'for-direction',\n",
       " 'no-compare-neg-zero',\n",
       " 'no-sparse-arrays',\n",
       " 'no-func-assign',\n",
       " 'no-const-assign',\n",
       " 'no-global-assign',\n",
       " 'use-isnan',\n",
       " 'no-unused-labels',\n",
       " 'require-yield',\n",
       " 'getter-return',\n",
       " 'no-dupe-class-members',\n",
       " 'no-ex-assign',\n",
       " 'constructor-super',\n",
       " 'no-new-symbol',\n",
       " 'no-empty-pattern',\n",
       " 'no-class-assign']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Warning types example\n",
    "all_warning_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc31419b-d1fa-4aa6-8afe-4d3450ceb306",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A data sample\n",
    "data[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a73d261-058c-4427-87fe-599e4d49848e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Unexpected 'this'.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].linter_report.message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2812d3f-942d-4061-99a6-293645757d51",
   "metadata": {},
   "source": [
    "## Write data back to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc2f7b8e-7558-489b-a1a8-1564016fa466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_train_data(sample):\n",
    "    return { \n",
    "        \"source_code\": sample.source_code,\n",
    "        \"target_code\": sample.target_code,\n",
    "        \"message\": sample.linter_report.message\n",
    "    }\n",
    "\n",
    "extracted_data = list(map(extract_train_data, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45bbb329-42b5-4bd1-a235-6e9ae0432636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../simplified-data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(extracted_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4711ab66-9220-4f70-b5c9-316393793e43",
   "metadata": {},
   "source": [
    "## Load simplified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a40f4ee-422a-4d24-98c0-6e81619ff1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0287fcd8e447188f3efdc70e623982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('json', data_files='../simplified-data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9fefc0-251f-4a15-9dd3-ef910b805d59",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "867fc5a2-60e4-492b-9075-7f05652dcfe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68136ca0f9314f9eaae60e5cbf288f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb929687-b039-4e1a-98f3-471a284e4a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare LoRA and apply to the model\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank of the LoRA update matrices\n",
    "    lora_alpha=32, # Scaling factor for the LoRA update matrices\n",
    "    lora_dropout=0.05, # Dropout probability for the LoRA update matrices\n",
    "    bias=\"none\", # Whether to apply a bias to the LoRA update matrices\n",
    "    task_type=\"CAUSAL_LM\" # Type of task for which to apply LoRA\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b62d5adc-3e07-4bf7-b1b1-304c457f9e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9d05adfa-fddd-4057-81a9-2b3e6c0d3054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocess function - ERROR\n",
    "def preprocess_function(examples):\n",
    "    inputs = f\"{examples['message']}{examples['source_code']}\"\n",
    "    targets = f\"{examples['target_code']}\"\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(text_target=targets, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d1e20e1f-974e-4be4-9ed3-acc89b92578b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply preprocess function - ERROR\n",
    "tokenized_data = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=2, # Adjust according to your CPU cores\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8721791f-da10-4561-a4a2-82c20cf34215",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fine tune the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model-output\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,  # use mixed precision if supported\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data['train'],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a846c23-18ec-4556-8989-21631e9a98b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare the training data - worked\n",
    "def preprocess_function(batch):\n",
    "\n",
    "    inputs = [f\"Input: {text}\" for text in batch['source_code']]\n",
    "    targets = [f\"Output: {text}\" for text in batch['target_code']]\n",
    "    print(inputs, targets)\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(text_target=targets, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_data = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=2,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model-output\",  # Updated output directory\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    ")\n",
    "\n",
    "# Create a Trainer instance and train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8359ed-f0bf-473a-9cfc-4aaeba0346db",
   "metadata": {},
   "source": [
    "# Generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b7cced7-0a1e-496d-b65d-a2e4253ed9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170965d45a4248908b55585b0682f532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_model_path = '../main/meta-llama/Llama-3.2-3B_global_28-10-2024_23-21-36_fixer_trained'\n",
    "\n",
    "# Load trained model\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(trained_model_path)\n",
    "\n",
    "# Load the LoRA adapter\n",
    "config = LoraConfig.from_pretrained(trained_model_path) \n",
    "trained_model = PeftModel.from_pretrained(trained_model, trained_model_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df5f00f1-79f6-431e-8468-3bff8f0b19bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2c8f590-f307-4094-99ea-7123e392559d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected an object to be thrown. throw 'Could not create XML HTTP transport.';\n"
     ]
    }
   ],
   "source": [
    "# Prepare input text\n",
    "prompt = \"Expected an object to be thrown. throw 'Could not create XML HTTP transport.';\"\n",
    "\n",
    "# Encode the input text\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "output = trained_model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode the output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c6b29-f3ae-42b4-9488-3d9b0f6c7502",
   "metadata": {},
   "source": [
    "# Compare with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5948be46-0d41-4c8a-9900-8472a64c4b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98fafac03ff4cee9ff5bff0009fa4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_path = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "# Load trained model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_path, token=hf_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path, token=hf_token)\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank of the LoRA update matrices\n",
    "    lora_alpha=32, # Scaling factor for the LoRA update matrices\n",
    "    lora_dropout=0.05, # Dropout probability for the LoRA update matrices\n",
    "    bias=\"none\", # Whether to apply a bias to the LoRA update matrices\n",
    "    task_type=\"CAUSAL_LM\" # Type of task for which to apply LoRA\n",
    ")\n",
    "base_model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49c2471d-daf9-481c-b992-12339e9fbc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected an object to be thrown. throw 'Could not create XML HTTP transport.'; at Object.createXmlHttpRequest (/usr/local/lib/node_modules/express/node_modules/connect/node_modules/http-proxy/node_modules/xmlhttprequest/index.js:4:20) at Object.exports.create (/usr/local/lib/node_modules/express/node_modules/connect/node_modules/http-proxy/node_modules/xmlhttprequest/index.js:11:19) at Object.exports.create (/usr/local/lib/node_modules/express/node_modules/connect/node_modules/http-proxy/index.js:33:12) at Object.exports.create (/usr/local/lib/node_modules/\n"
     ]
    }
   ],
   "source": [
    "# Prepare input text\n",
    "prompt = \"Expected an object to be thrown. throw 'Could not create XML HTTP transport.';\"\n",
    "\n",
    "# Encode the input text\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "output = base_model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode the output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a5f72e-94bb-4958-98db-82cdf2f97860",
   "metadata": {},
   "source": [
    "# Compare with instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2777b19-c0a1-447a-b67d-7be0fd231f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9953d719f0914609af1464676ea7ddb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fix] throw new Error('Could not create XML HTTP transport.')\n"
     ]
    }
   ],
   "source": [
    "instruct_model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(instruct_model_id)\n",
    "instruct_model = AutoModelForCausalLM.from_pretrained(\n",
    "    instruct_model_id,\n",
    "    token=hf_token,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a JavaScript syntax error correction expert. You will receive code snippets with syntax errors, prefixed with [Fix]. Each input will have two parts separated by a vertical bar (|). You will only fix the provided code, without any additional explanation.\"},\n",
    "    {\"role\": \"user\", \"content\": \"[Fix]Expected an object to be thrown| throw 'Could not create XML HTTP transport.'\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(instruct_model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = instruct_model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16174e1-e382-4c27-9104-1aeede3ed1cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ab3fa507-52d9-4c66-9d23-e6c3a77ba16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Input: def add(a, b): return a + b Output: Output: def add(a, b): return a - b Explanation: The output is the difference between the two numbers. The output is the sum of the two numbers. The output is the product of the two numbers. The output is the quotient of the two numbers. The output is the absolute value of the difference between the two numbers. The output is the absolute value of the sum of the two numbers. The output is the absolute value of the product of the two\n"
     ]
    }
   ],
   "source": [
    "# Function to generate code\n",
    "def generate_code(input_code):\n",
    "    inputs = tokenizer(f\"Input: {input_code}\", return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_code\n",
    "\n",
    "# Example usage\n",
    "input_code = \"Input: def add(a, b):\"\n",
    "generated_code = generate_code(input_code)\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d665fb9b-9778-4846-be0b-449e705ae147",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
